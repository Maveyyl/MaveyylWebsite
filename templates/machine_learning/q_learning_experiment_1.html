<h3>Experiment 1</h3>
<div class="row">
<div class="col-md-3">
	{{updates_per_second}} updates per second
	<br>
	<button (click)="speed_divided_by_2()">speed / 2</button>
	<button (click)="speed_times_2()">speed x 2</button>
	<br>
	<input type="checkbox" [(ngModel)]="render"> Toggle rendering
	<br>

	<div [hidden]="!render" id="experiment-2-phaser"></div>

	<svg id="nn-graph"></svg>
</div>

<div class="col-md-9">
	<div class="row">
	<div class="col-md-4">
		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G1: Average Error</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-error"></svg>
			</div>
		</div>

		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G2: Average time spent exploring instead of exploiting</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-random-behavior"></svg>
			</div>
		</div>

	</div>

	<div class="col-md-4">
		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G3: Average reward</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-reward"></svg>
			</div>
		</div>

		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G4: Average amount of time plant was nearby</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-plant-nearby"></svg>
			</div>
		</div>

	</div>
	
	<div class="col-md-4">
		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G5: Average amount of time plant was eaten when nearby</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-eat"></svg>
			</div>
		</div>
		
		<div class="panel panel-default">
			<div class="panel-heading">
				<h3 class="panel-title">G6: Average amount of time plant was fed when nearby.</h3>
			</div>
			<div class="panel-body">
				<svg id="avg-feed"></svg>
			</div>
		</div>
	</div>
	</div>

	<div class="row">
	<div class="col-md-9">
		<br>
		<small> The following is an attempt to explain everything. There is a LOT of text, english is not my mother tongue and the subject is really difficult.</small>
		<br>
		<br>
		<h4>What is displayed?</h4>
		<p>
			This is a simulation an Artificial Intelligence living in a simple world where it can sense its environment and act on it.<br>
			<br>
			On the left are two buttons and one checkbox, click on the checkbox to see the rendering of the simulation.<br>
			Under the rendering is the representation of the neural network of the AI.<br>
			Center and left are graphes showing what decisions are taken by the AI on the average.<br>
		<p>
		<br>




		<h4>Rules of the simulation</h4>
		<p>
			The AI is called the "creature" <img src="/img/QLearningExperiment1/creature.png">. It lives in a 2 dimensional tiled world where time is turn based.<br>
			It is able to move from tile to tile in 4 different directions: left, top, right and bottom, and can see the objects that are in the adjacent tiles: nothing <img src="/img/QLearningExperiment1/ground.png">, small crystal <img src="/img/QLearningExperiment1/small_plant.png">, medium crystal <img src="/img/QLearningExperiment1/medium_plant.png">, or big crystal <img src="/img/QLearningExperiment1/big_plant.png">. Cristals are randomly spawned in the world at the beginning. In addition, the creature can also see objects 3 tiles ahead in all 4 directions but can't tell which kind of object it is.<br>
			The ultimate goal of the creature is to eat the crystals. For that, the creature has two actions: feed and eat. Feed will increase the size of nearby crystals, a small crystal can be fed two times to become a big crystal. Eat will consume the surrounding crystals, the bigger they are, the more food the crystals will provide. The creature can also choose to do nothing.<br>
			<br>
			To summary the creature has 7 possible actions: move left, move top, move right, move bottom, do nothing, feed, eat. Some actions in certain circumstances will do nothing, such as eating while no crystals is around, or moving to a tile that has already an object on it.<br>
			The creature has 16 sensors: small crystal presence for all 4 directions, medium crystal presence for all 4 directions, big crystal presence for all 4 directions, object presence 3 tils afar for all 4 directions.<br>
			Every action that is chosen, the creature will be given a reward value: 0 by default, 100/200/300 per small/medium/big crystal eaten.<br>
		</p>
		<br>


		<h4>How can we interact with the simulation?</h4>
		<ul>
			<li>Click on "Toggle rendering" checkbox to see/hide the simulation in real time.</li>
			<li>Click on "speed / 2" or "speed * 2" to increase or decrease by 2 the simulation's updates done per second.</li>
			<li>Admire or not.</li>
			<li>Watch the representation of the Neural Network of the AI, red for negative links, blue for positive links, wider link has more impact on the decision making.</li>
			<li>Watch nice graphs because data is beautiful. They won't hurt the performance of your browser in long runs.</li>
		</ul>
		<br>

		<h4>How does it work?</h4>
		<p>
			In short, that is Q learning with Neural Network. See the Links page of this website for good links on matter.<br>
			<br>
			The creature will take decisions thanks to its artificial Neural Network (NN). The NN is designed to take in input the value of the creature's sensors, and will output one value par possible action that represents the potential immediate or future reward for applying the given action.<br>
			<br>
			At first this NN isn't trained and doesn't know how to take good decisions toward the surrounding environment. It'll explore the environment by applying random actions. A reward will be given for an applied action and the creature will tweak its NN into outputting that reward for the given sensors state and the choosen action. That works just like how we learn, we estimate something we don't know, we confront it with reality, and learn from our error.<br>
			<br>
			Now you're supposed to wonder how it can possibly learn that feeding a crystal can possibly lead to eating more. That is because of the way we compute the reward. We first compute the base reward considering the state of the creature, the action and the environment, the creature applies the action and its state is changed, it then predicts the potential reward for all possible actions, selects the action that leads to the best reward and adds a part of this reward to the previously computed base reward.<br>
			reward = base reward + discount factor * best next reward <br>
			For example, if our discount factor is 0.6, if the creature is moving next to a food piece, it'll have a base reward of 0, added to it 0.6 * 100 because the best next action is eating the food piece, which gives 100 as reward. Right here it only gives insight of potential reward 1 turn forward, but with repetition the potential reward added here 0.6 * 100 will be discounted and added to previous steps, which will give insight of potential rewards many turns forward.<br>
			<br>
			The creature will gradually reduce the frequency with which it's taking random decisions and will start choosing decisions according to the best potential reward. At some point it'll take its own decisions 95% of the time, it's important to keep taking random decisions sometimes to ensure that the creature doesn't get stuck and continue exploring.<br>
		</p>
		<br>

		<h4>Expected results</h4>
		<p>
			It is expected that the creature slowly learns to move toward distant crystals, feed them and eat them.<br>
		</p>
		<br>

		<h4>Results</h4>
		<p>
			The creature consistently learns what effect an action will have on a given state, and so it'll avoid actions that lead to nothing.<br>
			It learns to move toward distant objects, it is difficult to say if the creature effectively learns to move toward the closest objects when there's a choice.<br>
			It learns to feed crystals when nearby and then eat them. This is a satisfying result, eating a small crystal or feeding the crystal and eating it will offer 100 reward per turn. Doing this doesn't provide any direct gain. What brings value to the principle of feeding is that the creature will spend less time moving and exploring, that means the creature learned how to optimize its average reward indirectly.<br>
			<br>
		</p>
		<br>

		<h4>Key parameters</h4>
		<ul>
			<li>16x20x7 sized Neural Network. 16x7 also works but sometimes fails to converge immediatly.</li>
			<li>ReLU as activation function and MSE as cost function</li>
			<li>Xavier initialization for the weights</li>
			<li>0.00003 learning rate</li>
			<li>0.8 discount factor</li>
			<li>0 regularization parameter</li>
			<li>10 memory replay size. Increases convergence speed and stability, doesn't change significantly anything if set high.</li>
			<br>
		</ul>
		<br>


		<h4>What was used to do this</h4>
		<ul>
			<li>Feedforward Neural Network with Backpropagation algorithm, implemented in JS</li>
			<li>Q learning algorithm adapted to Neural Network</li>
			<li>Phaser for the rendering of the simulation</li>
			<li>D3JS for the graphes and neural network representation</li>
		</ul>

	</div>
	</div>
</div>
</div>
